---
title: "TITLE OF YOUR PROJECT"
author: "Group 3"
date: "November 12th, 2024"
output: pdf_document
---

List your group members, including their student numbers, here:

-   Noya Barak (169097527)

-   Sara Haifa (169087012)

-   Satinder Kaur (169109308)

-   Spencer Mozeg (169099531)

-   Khushi Satra, (203383100)

```{r setup, include=FALSE}
# echo = FALSE will set the Rmd to *not* show the R code. Don't change this.
# You may change the default figure width and figure height as you please.
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.width = 6)

# Put any libraries that you need to load here.
# DO NOT PUT "install.packages()" IN AN RMD FILE!!!
library(tidyverse)
library(arrow)
```

# Instructions

You will only submit the PDF version of this document. To knit to PDF, you'll need to run `install.packages("tinytex")` in the console, followed by `tinytex::install_tinytex()` (DO NOT PUT THESE COMMANDS IN AN RMD FILE!!!). If you encounter errors in "Knit to PDF", you can "knit to html" and then print the html file to PDF using your operating system's PDF view (e.g. Adobe Acrobat). Only standalone PDF files will be accepted by MyLS.

# Abstract

The analysis presented is a continuation of our exploration of Disparate Data: Part 1. We explore how three disitnct data sets might have a relationship, and we do so by using R code.

This report aims to explore the correlation of.... (1), (2), (3)..

# Introduction

In 2020, global events such as the COVID-19 pandemic reshaped societal priorities, affecting public health, economic stability, and general well-being worldwide. This study explores the interrelations between national happiness levels, public awareness of climate change, and COVID-19 case data to understand broader implications on public sentiment and awareness, crossing over from the Covid 2020 data set, and Climate Awareness, and Happiness.

We imported and tidied the datasets using techniques to ensure they combined seamlessly. The `dplyr` package was used for data manipulation (e.g., `filter()`, `mutate()`, `group_by()`), and `pivot_longer()`/`pivot_wider()` for reshaping. We standardized column names with `janitor::clean_names()` and handled missing values with `na.rm = TRUE`. Finally, the cleaned data was exported using `write_parquet().`

In this report, we are going to explore some aspects climate change and the impact and/or perceptions of it by using exploratory techniques. We'll explore \<<general description of data>\> using \<<general description of techniques>\>.

By the end of this report, we will have shown ...

# Data Description

## 1. Climate Awareness

```{r load_data1}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1

# Note that the code in this document will not be shown
# when you click "knit", so the placement of this code
# chunk is purely for your benefit: You can see what happened
# with your data, which makes it easier to describe below!

climate_opinion_address <- "https://data.humdata.org/dataset/dc9f2ca4-8b62-4747-89b1-db426ce617a0/resource/6041db5f-8190-47ff-a10b-9841325de841/download/climate_change_opinion_survey_2022_aggregated.xlsx"

climate_sheet_names <- climate_opinion_address |>
    loadWorkbook() |>
    names()

aware_sheet_name <- "climate_awareness"

climate_awareness <- climate_opinion_address |>
    read.xlsx(
        sheet = aware_sheet_name
    ) |>
    pivot_longer(
        cols = !contains(aware_sheet_name),
        names_to = "country",
        values_to = "score"
    ) |>
    mutate(
        climate_awareness = case_when(
            climate_awareness == "I have never heard of it" ~ "aware_no",
            climate_awareness == "I know a little about it" ~ "aware_alittle",
            climate_awareness == "I know a moderate amount about it" ~
                "aware_moderate",
            climate_awareness == "I know a lot about it" ~ "aware_alot",
            climate_awareness == "Refused" ~ "aware_refuse",
            climate_awareness == "(Unweighted Base)" ~ "aware_base"
        )
    ) |>
    rename(answer = climate_awareness) |>
    pivot_wider(
        names_from = answer,
        values_from = score
    )

write_parquet(climate_awareness, "climate_awareness.parquet")

climate_awareness

#Extra tidying 
```

The data come from [a climate awareness surve](https://data.humdata.org/dataset/dc9f2ca4-8b62-4747-89b1-db426ce617a0/resource/6041db5f-8190-47ff-a10b-9841325de841/download/climate_change_opinion_survey_2022_aggregated.xlsx)y ***(Add footnote here for references)*** and describes countries awareness of climate change, grouping them into people who are knowledgeable about climate change to people who's never heard of it. It comes from an Excel document with other related climate topics, brought from the [2020 survey](https://dataforgood.facebook.com/dfg/tools/climate-change-opinion-survey).

After loading in the data from the Excel sheet, we separated the country and score column and simplified the survey answers to clearer headings.

After loading in the data from the Excel sheet, pivoting was used to reshape the data from a wide format (with each survey response category in separate columns) to a long format that is easier to analyze the score of countries. Then, we renamed the survey options to shorter codes simplifies the data for future processing and analysis. Using "aware_no" instead of lengthy descriptions such as "I have never heard of it" makes the code more concise and readable. The climate_awareness column was renamed as well to ensure clarity and consistency in column naming. Next, we restructured the data so that survey response categories are column names (e.g., aware_no, aware_alittle) and their respective scores are values, making it easier to analyze each country's awareness level. Finally, the cleaned data was saved as a a.parquet file.

## 2. COVID-19 2020 

```{r load_data2}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1

# Reminder: do NOT print your data to the screen unless it's
# completely necessary
#-----------------------------------------------------------------------------------

knitr::opts_chunk$set(error = TRUE)
library(tidyverse)
theme_set(theme_bw())
library(arrow)
library(openxlsx)

owid_address <- "https://covid.ourworldindata.org/data/owid-covid-data.csv"
covid_cases <- owid_address |>
    read_csv(
        col_types = cols(
            .default = col_double(),
            date = col_date(format = ""),
            iso_code = col_character(),
            location = col_character(),
            continent = col_character(),
            tests_units = col_character()
        ),
        progress = FALSE,
        show_col_types = FALSE
    )

covid_cases

covid_2020 <- covid_cases|>
  filter(year(date) == 2020 )|>
  group_by(country = location, continent) |>
  summarise(total_cases = sum(total_cases, na.rm = TRUE),
            .groups = "drop")|>
  select(country, continent, total_cases)
  

covid_2020

write_parquet(covid_2020, "covid_2020.parquet")
```

Collected from [here](https://covid.ourworldindata.org/data/owid-covid-data.csv), this data set showcases the cases in each country (as well as its continent), reported in 2020. It contains a wide range of information as well (like life expectancy, total_deaths, etc.)

When we tidied up the data, we focused specifically on covid cases. Firstly, after loading out the data, we defined the columns with various functions, to ensure correct data types for example. Then we filtered by year as every collection was from the same year but different periods of 2020, which just made the data more cleaner. Next, we grouped location and continent to connect the pairs, summarized total cases to sum up the cases, and removed NA to ensure correct values. After that, we selected country, continent, and total_cases as those were our main columns we wanted to focus on.

## 3. World Happiness Report Score (from 2023 Report)

```{r load_data2}
# Put in your code to load in the data set, along with any
# necessary cleaning beyond what was done in Part 1

happiness <- read.xlsx("DataForTable2.1.xlsx") |>
    janitor::clean_names() |>
    rename(
      country = 'country_name'
    ) |>
  filter(
    !is.na(life_ladder)
  ) |
  group_by(country) |>
  slice_max(year, n=1)

happiness
```

This dataset was taken from [Home \| The World Happiness Report](https://worldhappiness.report/) (specifically used from [here](https://worldhappiness.report/ed/2023/#appendices-and-data)) and records the areas of 'happiness' in each country. The column closest to this measure is the `life_ladder` column; happiness in each country.

In Part 1, cleaning up life_ladder was focused on. Tidying up this column had the following steps. `Janitor` used to clean up the names (turns uppercase into lower case and replacing spaces or such with underscores), renaming the country column (from `country_name`), filtering out the missing/incomplete values (`NA's`), and lastly only focusing on recent observations.

happiness_covid <- covid_2020 |>
  inner_join(happiness, by = "country") 
happiness_covid


happiness_covid_altered <- happiness_covid |>
  group_by(country) |>
  mutate(
    total_covid_cases = sum(total_cases[year %in% c(2020, 2021, 2022, 2023, 2024)], na.rm = TRUE),
    corruption_base = case_when(
      perceptions_of_corruption < 0.3333 ~ "low corruption",
      perceptions_of_corruption >= 0.3333 & perceptions_of_corruption < 0.6666 ~ "medium corruption",
      perceptions_of_corruption >= 0.6666 ~ "high corruption"
    )
  ) |>
  ungroup()
  happiness_covid_altered_filtered <- happiness_covid_altered |>
  filter(
    total_covid_cases >= (quantile(total_covid_cases, 0.25) - 1.5*IQR(total_covid_cases)) &
    total_covid_cases <= (quantile(total_covid_cases, 0.75) + 1.5*IQR(total_covid_cases))
  )
happiness_covid_plot <- happiness_covid_altered_filtered |>
  ggplot(aes(x = log_gdp_per_capita, y = total_covid_cases, color = corruption_base)) +
  geom_point(size = 1.2, alpha = 0.7) + 

  scale_color_manual(values = c("low corruption" = "blue", "medium corruption" = "orange", "high corruption" = "red")) +
  labs(
    x = "Log GDP per Capita",
    y = "Total COVID-19 Cases (Log Scale)",
    color = "Corruption Level",
    title = "COVID-19 Total Cases vs. GDP per Capita by Corruption Level",
    caption = "Each point represents a country, colored by corruption level"
  ) +
  facet_wrap(~continent) +
  theme_minimal() 
                        
happiness_covid_plot

Explain how any combinations of data were performed. Explain what kind of join was needed, whether columns had to be modified (for example, matching "country" names.)

1. We performed the combination of the data through the inner_join function
2. This function helps recognize which countries are mutual between the two datasets, and therefore we do not have any missing data per country due to merging
3. The only column that we modifed was the year column, since it only provided covid statistics per individual year, rather than reporting a statistic of the net cases per country
4. The plot that we made thus combined information from both the covid dataset (continent data and total covid cases) and data from the happiness dataset (gdp per capita and corruption rates)
5. Through this, we can see how countries who overall generate less capital per person's population is affected by covid, and also if certain continents in the world's covid rates were higher
6. Additionally, 

summary_table <- happiness_covid_altered_filtered |>
  group_by(corruption_base) |>
  summarize(
    mean_total_cases = mean(total_covid_cases, na.rm = TRUE),
    median_total_cases = median(total_covid_cases, na.rm = TRUE),
    sd_total_cases = sd(total_covid_cases, na.rm = TRUE),
    mean_log_gdp = mean(log_gdp_per_capita, na.rm = TRUE),
    median_log_gdp = median(log_gdp_per_capita, na.rm = TRUE),
    sd_log_gdp = sd(log_gdp_per_capita, na.rm = TRUE),
    mean_happiness = mean(happiness_score, na.rm = TRUE),
    median_happiness = median(happiness_score, na.rm = TRUE),
    sd_happiness = sd(happiness_score, na.rm = TRUE)
  )
summary_table |>
  kable(
    caption = "Summary Statistics by Corruption Level",
    col.names = c("Corruption Level", 
                  "Mean Total Cases", "Median Total Cases", "SD Total Cases", 
                  "Mean Log GDP", "Median Log GDP", "SD Log GDP",
                  "Mean Happiness", "Median Happiness", "SD Happiness")
  ) 
# Exploratory Data Analysis

To achieve our goals, we explored the data by visualization specific information into plots, and recognizing its patterns.

We explored many aspects of the data, but will demonstrate three. These are \<\<insight 1\>\>, \<\<insight 2\>\>, and \<<insight3>\>

The first aspect that we found interesting is shown in \@ref(fig:insight1). The insight should be specific to the data shown, not a general statement beyond the data (leave that for the conclusion).

```{r insight1, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This is an example of how you can control figures and captions in
# an R chunk. Note that you can reference figures using:
# \@ref(fig:insight1), where "insight1" is the label of this code
# chunk (the first bit of text after the "r" in "```{r label, options...}")

aware_happiness <- inner_join(climate_awareness_update2, happiness, by = "country")|>
rename( awareness_level = answer)|>
  arrange(desc(log_gdp_per_capita))|>
  head(30)
  
aware_happiness
```
plot_aware_happiness<-
 ggplot(aware_happiness, aes(x= country, y= life_ladder, fill = score ))+
  geom_bar(stat =  "identity")+
  geom_smooth( method = "lm")+
  facet_wrap(~awareness_level)+
   
   labs(title = "Happiness of People based on their climate awareness",
        x = "Countries with highest GDP per capita",
        y = "Ranking based on average happiness/ life satisfaction")+
   theme(axis.text.x = element_text(angle = 90))

plot_aware_happiness

This insight is supported by the summary statistics in table \@ref(tab:summary_stats)

```{r summary_stats}
# Calculate the relevant summary statistics here.
# Note that the "kable" function in the "knitr" package
# is convenient for making nice tables. Other packages can
# do much fancier things with tables, but keep in mind that
# the insights should be the star, not the formatting.
```

The next insight that we found is shown in \@ref(fig:insight2).

```{r insight2, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
# This figure will have a height of 4 and a width of 6.
# Feel free to change this, and to apply different sizes
# to the other figures you create.


```

Finally, \@ref(fig:insight3) shows ...

```{r insight3, fig.height=4, fig.width=6, fig.cap="This is a figure caption that you will need to change in order to get good marks in the visualization rubric items."}
```

# Conclusion and Future Work

Overall, we found \<<general ideas>\>.

A second paragraph about our findings.

The next steps in this analysis are...

The limitations of this analysis are as follows. (Do not simply list potential issues with sampling, but relate them to your analysis and how they affect your conclusions. An honest and complete acknowledgement of the limitations makes the analysis more trustworthy.)

# References

I am not strict about MLA or APA style or anything like that. For this report, I would much rather have your citations be easy to match to your insights.

The easiest way is to use Rmd's [footnote](https://bookdown.org/yihui/rmarkdown/markdown-syntax.html#inline-formatting) syntax. This will put a number beside the word where the footnote appears, and the full text of the footnote at the bottom of the page (pdf) or end of the document (html). The syntax is:[^1], where I suggest that you put in something like this[^2] to make references for this assignment.

[^1]: See the source view to see this footnote

[^2]: The relevance to the insight is ... . From \<<name of source and name of article>\>, published on \<<date>\>, url: \<<link to page>\>

Alternatively, you could make a list of citations with their main arguments and why they're relevent to your insights, methods, etc.

The link above also references "bibtex" files. These are also extremely convenient, but have a steep learning curve and they make it difficult to tie them to an insight. If you use bibtext, then make sure that you provide a sentence to describe the source and it's relevance when you cite it - don't just add citations to the end of a sentence (this is common practice in academia, but I want to know that your citations are directly relevant for this assignmnet).
